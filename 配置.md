# 服务器

|  名称  |     内网ip     |    外网ip     |
| :----: | :------------: | :-----------: |
| cloud1 | 172.26.154.146 | 39.99.134.147 |
| cloud2 | 172.26.240.217 | 39.98.134.47  |
| cloud3 | 172.26.154.144 | 39.99.134.117 |
| cloud4 | 172.26.154.145 | 39.99.140.220 |
| cloud5 | 172.26.240.216 | 39.100.90.186 |



# 节点分配

|           |                 | cloud1 | cloud2 | cloud3 | cloud4 | cloud5 |
| :-------: | :-------------: | :----: | :----: | :----: | :----: | :----: |
| Zookeeper | QuorumPeerMain  |   √    |   √    |   √    |        |        |
|   HDFS    |    NameNode     |   √    |   √    |        |        |        |
|           |   JournalNode   |   √    |   √    |   √    |        |        |
|           |    DataNode     |   √    |   √    |   √    |        |        |
|           |      ZFKC       |   √    |   √    |        |        |        |
|   YARN    | ResourceManager |   √    |   √    |        |        |        |
|           |   NodeManager   |   √    |   √    |   √    |   √    |   √    |
|   Hbase   |     HMaster     |   √    |   √    |        |        |        |
|           |  HRegionServer  |   √    |   √    |   √    |        |        |
|   Kafka   |     Worker      |        |        |   √    |   √    |   √    |
|   Spark   |     Broker      |   √    |   √    |   √    |   √    |   √    |
|  MongoDB  |     Master      |        |        |        |   √    |   √    |
|           |    Secondary    |        |        |   √    |        |        |
|  Django   |        -        |        |        |        |   √    |   √    |
|   Redis   |        -        |        |        |        |   √    |   √    |
|   Nginx   |        -        |        |        |        |   √    |   √    |



# 重要端口(部分自己修改过)

|   软件    | 端口 |       节点       |     说明      |
| :-------: | :--: | :--------------: | :-----------: |
| Zookeeper | 6181 |      Server      |  客户端请求   |
|           | 6888 |      Server      |     通信      |
|           | 6889 |      Server      |     选举      |
|   HDFS    | 8020 |     NameNode     |    rpc服务    |
|           | 9870 |     NameNode     |   http服务    |
|   YARN    | 8088 | ResourceManager  |   http服务    |
|           | 8040 |   NodeManager    |    ipc服务    |
|           | 8042 |   NodeManager    |   http服务    |
|   Hbase   | 9000 |     HMaster      |     连接      |
|           | 9001 |     HMaster      |   http服务    |
|           | 9002 |  HRegionServer   |     连接      |
|           | 9003 |  HRegionServer   |   http服务    |
|           | 9090 |   ThriftServer   | happybase连接 |
|   Spark   | 7077 |      Master      |   提交任务    |
|   Kafka   | 9092 |      Broker      |     连接      |
|  MongoDB  | 8018 | Master,Secondary |     连接      |
|   Redis   | 6379 |        -         |     连接      |
|  Django   | 8004 |        -         |   访问服务    |
|   uWSGI   | 8005 |        -         |   访问服务    |
|   Nginx   | 9006 |        -         |   访问服务    |



# 修改 HOSTNAME

## 在 `cloud1` 上，修改 `HOSTNAME`


1. 设置 `HOSTNAME` 

```
hostnamectl set-hostname cloud1
```

2. 修改 `hosts` 文件

```
vim /etc/hosts
```


```
0 cloud1
172.26.240.217 cloud2
172.26.154.144 cloud3
172.26.154.145 cloud4
172.26.240.216 cloud5
```

## 在 `cloud2`、`cloud3`、`cloud4`、`cloud5` 上，修改 `HOSTNAME`



# 配置免密钥登录

## 配置 `cloud1` 免密登录 5 台服务器

1. 生成密钥

```
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
```

2. 拷贝公钥


```
scp ~/.ssh/id_rsa.pub cloud2:/root/.ssh/id_rsa.pub.cloud1
scp ~/.ssh/id_rsa.pub cloud3:/root/.ssh/id_rsa.pub.cloud1
scp ~/.ssh/id_rsa.pub cloud4:/root/.ssh/id_rsa.pub.cloud1
scp ~/.ssh/id_rsa.pub cloud5:/root/.ssh/id_rsa.pub.cloud1
```

3. 在 `cloud1` 上，将公钥写入 `authorized_keys` 文件


```
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

4. 在 `cloud2`、`cloud3`、`cloud4`、`cloud5` 上，将公钥写入 `authorized_keys` 文件

```
cat ~/.ssh/id_rsa.pub.cloud1 >> ~/.ssh/authorized_keys
```

## 配置 `cloud2`、`cloud3`、`cloud4`、`cloud5` 免密登录 5 台服务器



# 安装 python3

## 安装 `python3`

```
yum install python3
```

## 删除原 `python` 和 `pip` 的软连接

```
cd /usr/bin
rm python pip
```

## 创建新的软连接

```
ln -s python3 python
ln -s pip3 pip
```



# 配置 JDK

## 解压安装包

```
tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/cloud302
mv /usr/cloud302/jdk1.8.0_231 /usr/cloud302/java
```

## 添加环境变量

```
vim /etc/profile
```

```
export JAVA_HOME=/usr/cloud/java 
export JRE_HOME=${JAVA_HOME}/jre  
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  
export PATH=${JAVA_HOME}/bin:$PATH
```

```
source /etc/profile
```

## 检查 `JDK`

```
java -version
```



# 配置 Zookeeper

## 解压安装包

```
tar -zxvf apache-zookeeper-3.5.6-bin.tar.gz -C /usr/cloud302
mv /usr/cloud302/apache-zookeeper-3.5.6-bin /usr/cloud302/zookeeper
```

## 添加环境变量

```
vim /etc/profile
```

```
export ZOOKEEPER_HOME=/usr/cloud302/zookeeper
export PATH=${ZOOKEEPER_HOME}/bin:$PATH
```

```
source /etc/profile
```

## 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `Zookeeper` 

1. 在 `cloud1`、`cloud2`、`cloud3` 上，创建 `data` 和 `logs` 目录

```
mkdir ${ZOOKEEPER_HOME}/{data,logs}
```

2. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `zoo.cfg` 文件


```
mv $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg
vim $ZOOKEEPER_HOME/conf/zoo.cfg
```

```
# 通信心跳时间
tickTime=2000
# 初始连接最大容忍心跳数
initLimit=10
# 连接后最大容忍心跳数
syncLimit=5
# 数据目录
dataDir=/usr/cloud302/zookeeper/data
# 日志目录
dataLogDir=/usr/cloud302/zookeeper/logs
# 客户端请求的端口
clientPort=6181
# 集群信息配置  server.编号=地址:通信端口:选举端口
server.1=cloud1:6888:6889
server.2=cloud2:6888:6889
server.3=cloud3:6888:6889
```

3. 在 `cloud1` 上，配置 `myid` 文件

```
echo '1' > $ZOOKEEPER_HOME/data/myid
```

4. 在 `cloud2` 上，配置 `myid` 文件

```
echo '2' > $ZOOKEEPER_HOME/data/myid
```
5. 在 `cloud3` 上，配置 `myid` 文件

```
echo '3' > $ZOOKEEPER_HOME/data/myid
```



# 配置 HDFS(HA)

## 解压安装包

```
tar -zxvf apache-zookeeper-3.5.6-bin.tar.gz -C /usr/cloud302
mv /usr/cloud302/hadoop-3.2.1 /usr/cloud302/hadoop
```

## 添加环境变量

```
vim /etc/profile
```

```
export HADOOP_HOME=/usr/cloud302/hadoop
export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH
```

```
source /etc/profile
```

## 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `HDFS(HA)`

1. 配置 `Zookeeper`

2. 确定节点分配


```
namenode: cloud1、cloud2
datanode: cloud1、cloud2、cloud3
journalnode: cloud1、cloud2、cloud3
```


3. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `hadoop-env.sh` 文件


```
vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
```

```
export JAVA_HOME=/usr/cloud302/java
```

4. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `start-dfs.sh` 文件

```
vim $HADOOP_HOME/sbin/start-dfs.sh
```

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_ZKFC_USER=root
```

5. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `stop-dfs.sh` 文件

```
vim $HADOOP_HOME/sbin/stop-dfs.sh
```

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_ZKFC_USER=root
```

6. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `core-site.xml` 文件

```
vim $HADOOP_HOME/etc/hadoop/core-site.xml
```

```
<configuration>
        <!-- 默认文件系统 -->
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://cluster</value>
        </property>
        <!-- 临时目录 -->
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/usr/cloud302/hadoop/tmp</value>
        </property>
        <!-- 指定任何ip可访问 -->
        <property>
                <name>hadoop.proxyuser.hduser.hosts</name>
                <value>*</value>
        </property>
        <!-- 指定任何用户可访问 -->
        <property>
                <name>hadoop.proxyuser.hduser.groups</name>
                <value>*</value>
        </property>
        <!-- zookeeper配置 -->
        <property>
                <name>ha.zookeeper.quorum</name>
                <value>cloud1:6181,cloud2:6181,cloud3:6181</value>
        </property>
</configuration>
```

7. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `hdfs-site.xml` 文件

```
vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```
<configuration>
        <!-- 关闭权限检查 -->
        <property>
                <name>dfs.permissions.enabled</name>
                <value>false</value>
        </property>
        <!-- namenode的本地储存地址 -->
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>usr/cloud302/hadoop/hdfs/name</value>
        </property>
        <!-- datanode的本地储存地址 -->
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/usr/cloud302/hadoop/hdfs/data</value>
        </property>
        <!-- journalnode的本地储存地址 -->
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/usr/cloud302/hadoop/hdfs/journal</value>
        </property>
        <!-- nameservice的逻辑名称 -->
        <property>
                <name>dfs.nameservices</name>
                <value>cluster</value>
        </property>
        <!-- namenode的逻辑名称 -->
        <property>
                <name>dfs.ha.namenodes.cluster</name>
                <value>nn1,nn2</value>
        </property>
		<!-- nn1的rpc监听地址 -->
        <property>
                <name>dfs.namenode.rpc-address.cluster.nn1</name>
                <value>cloud1:8020</value>
        </property>
        <!-- nn2的rpc监听地址 -->
        <property>
                <name>dfs.namenode.rpc-address.cluster.nn2</name>
                <value>cloud2:8020</value>
        </property>
        <!-- nn1的http地址 -->
        <property>
                <name>dfs.namenode.http-address.cluster.nn1</name>
                <value>cloud1:9870</value>
        </property>
        <!-- nn2的http地址 -->
        <property>
                <name>dfs.namenode.http-address.cluster.nn2</name>
                <value>cloud2:9870</value>
        </property>
        <!-- journal的uri地址 -->
        <property>
                <name>dfs.datanode.shared.edits.dir</name>
                <value>qjournal://cloud1:8485;cloud2:8485/cluster</value>
        </property>
        <!-- namenode故障转移的代理类 -->
        <property>
                <name>dfs.client.failover.proxy.provider.cluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <!-- 隔离方法 -->
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
        </property>
        <!-- ssh密钥 -->
        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/root/.ssh/id_rsa</value>
        </property>
        <!-- namenode自动故障转移 -->
        <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
        </property>
</configuration>
```

8. 在 `cloud1` 上，配置 `workers` 文件


```
vim /usr/hadoop/hadoop-3.2.1/etc/hadoop/workers
```

```
cloud1
cloud2
cloud3
```


# 配置 YARN(HA)

## 完成 `Hadoop` 环境配置

## 在 `cloud1`、`cloud2`、`cloud3`、`cloud4`、`cloud5` 上，配置 `YARN(HA)`

1. 确定节点分配

```
resourcemanager: cloud1、cloud2
nodemanager: cloud1、cloud2、cloud3、cloud4、cloud5

```
2. 在 `cloud1`、`cloud2`、`cloud3`、`cloud4`、`cloud5` 上，配置 `start-yarn.sh` 文件


```
vim $HADOOP_HOME/sbin/start-yarn.sh
```

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USRR=yarn
YARN_NODEMANAGER_USER=root
```

3. 在 `cloud1`、`cloud2`、`cloud3`、`cloud4`、`cloud5` 上，配置 `stop-yarn.sh` 文件


```
vim $HADOOP_HOME/sbin/stop-yarn.sh
```

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USRR=yarn
YARN_NODEMANAGER_USER=root
```

4. 在 `cloud1`、`cloud2`、`cloud3`、`cloud4`、`cloud5` 上，配置 `mapred-site.xml` 文件


```
vim $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

```
<configuration>
        <!-- 指定mr框架为yarn模式 -->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
</configuration>
```

5. 在 `cloud1`、`cloud2`、`cloud3`、`cloud4`、`cloud5` 上，配置 `yarn-site.xml` 文件


```
vim $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```
<configuration>
        <!-- nodemanager附属服务 -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
        <!-- 开启HA模式 -->
        <property>
                <name>yarn.resourcemanager.ha.enabled</name>
                <value>true</value>
        </property>
        <!-- 集群名称 -->
        <property>
                <name>yarn.resourcemanager.cluster-id</name>
                <value>cluster-yarn</value>
        </property>
        <!-- resouremanager名称 -->
        <property>
                <name>yarn.resourcemanager.ha.rm-ids</name>
                <value>rm1,rm2</value>
        </property>
        <!-- rm1对应服务器 -->
        <property>
                <name>yarn.resourcemanager.hostname.rm1</name>
                <value>cloud1</value>
        </property>
        <!-- rm2对应服务器 -->
        <property>
                <name>yarn.resourcemanager.hostname.rm2</name>
                <value>cloud2</value>
        </property>
        <!-- 自动恢复 -->
        <property>
                <name>yarn.resourcemanager.recovery.enabled</name>
                <value>true</value>
        </property>
		<!-- 设置为zookeeper模式 -->
        <property>
                <name>yarn.resourcemanager.store.class</name>
                <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        </property>
        <!-- zookeeper配置 -->
        <property>
                <name>hadoop.zk.address</name>
                <value>cloud1:6181,cloud2:6181,cloud3:6181</value>
        </property>
</configuration>

```

6. 在 `cloud1` 上，配置 `workers` 文件


```
vim /usr/hadoop/hadoop-3.2.1/etc/hadoop/workers
```

```
cloud1
cloud2
cloud3
cloud4
cloud5
```



# 配置 Hbase(HA)

## 解压安装包

```
tar -zxvf hbase-2.2.2-bin.tar.gz -C /usr/cloud302
mv /usr/cloud302/hbase-2.2.2 /usr/cloud302/hbase
```

## 添加环境变量

```
vim /etc/profile
```

```
export HBASE_HOME=/usr/cloud302/hbase
export PATH=${HBASE_HOME}/bin:$PATH
```

```
source /etc/profile

```

## 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `Hbase(HA)`

1. 配置 `HDFS(HA)`

2. 确定节点分配

```
hmaster: cloud1、cloud2
hregionserver: cloud1、cloud2、cloud3

```

3. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `hbase-env.sh` 文件

```
vim ${HBASE_HOME}/conf/hbase-env.sh

```

```
export JAVA_HOME=/usr/cloud302/java
export HBASE_MANAGES_ZK=false

```

4. 在 `cloud1`、`cloud2`、`cloud3` 上，拷贝 `HDFS` 的配置

```
cp $HADOOP_HOME/etc/hadoop/core-site.xml $HBASE_HOME/conf
cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HBASE_HOME/conf

```

5. 在 `cloud1`、`cloud2`、`cloud3` 上，配置 `hbase-site.xml` 文件

```
vim ${HBASE_HOME}/conf/hbase-site.xml

```

```
<configuration>
        <!-- hdfs路径 -->
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://cluster/hbase</value>
        </property>
        <!-- 开启完全分布式模式 -->
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>
        <!-- zookeeper服务器 -->
        <property>
                <name>hbase.zookeeper.quorum</name>
                <value>cloud1:6181,cloud2:6181,cloud3:6181</value>
        </property>
        <!-- 关闭容量检测 -->
        <property>
                <name>hbase.unsafe.stream.capability.enforce</name>
                <value>false</value>
        </property>
        <!-- master的端口 -->
        <property>
                <name>hbase.master.port</name>
                <value>9000</value>
        </property>
        <!-- master的web端口 -->
        <property>
                <name>hbase.master.info.port</name>
                <value>9001</value>
        </property>
        <!-- regionserver的web端口 -->
        <property>
                <name>hbase.regionserver.port</name>
                <value>9002</value>
        </property>
        <!-- regionserver的web端口 -->
        <property>
                <name>hbase.regionserver.info.port</name>
                <value>9003</value>
        </property>
        
        <!-- 设置thrift服务超时时间为一天 -->
        <property>
             <name>hbase.thrift.server.socket.read.timeout</name>
             <value>86400000</value>
        </property>

        <property>
                 <name>hbase.thrift.connection.max-idletime</name>
                 <value>86400000</value>
        </property>
</configuration>

```

6. 在 `cloud1` 上，配置 `regionservers` 文件

```
vim ${HBASE_HOME}/conf/regionservers

```

```
cloud1
cloud2
cloud3

```



# 配置 Kafka(HA)

## 解压安装包

```
tar -zxvf kafka_2.12-2.3.1.tgz -C /usr/cloud302
mv /usr/cloud302/kafka_2.12-2.3.1 /usr/cloud302/kafka
```

## 添加环境变量

```
vim /etc/profile
```

```
export KAFKA_HOME=/usr/cloud302/kafka
export PATH=${KAFKA_HOME}/bin:$PATH

```

```
source /etc/profile

```

## 在 `cloud3`、`cloud4`、`cloud5` 上，配置 `Kafka(HA)`

1. 配置 `Zookeeper`
2. 在 `cloud3` 上，配置 `server.properties` 文件

```
vim $KAFKA_HOME/config/server.properties

```

```
bloker.id=3
listeners=PLAINTEXT://cloud3:9092
advertised.listeners=PLAINTEXT://cloud3:9092
num.partitions=3
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3
zookeeper.connect=cloud1:6181,cloud2:6181,cloud3:6181

```
3. 在 `cloud4` 上，配置 `server.properties` 文件

```
vim $KAFKA_HOME/config/server.properties

```

```
bloker.id=4
listeners=PLAINTEXT://cloud4:9092
advertised.listeners=PLAINTEXT://cloud4:9092
num.partitions=3
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3
zookeeper.connect=cloud1:6181,cloud2:6181,cloud3:6181

```
4. 在 `cloud5` 上，配置 `server.properties` 文件

```
vim $KAFKA_HOME/config/server.properties

```

```
bloker.id=4
listeners=PLAINTEXT://cloud4:9092
advertised.listeners=PLAINTEXT://cloud4:9092
num.partitions=3
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3
zookeeper.connect=cloud1:6181,cloud2:6181,cloud3:6181

```



# 配置 Spark(On YARN)

## 解压安装包

```
tar -zxvf spark-2.4.4-bin-hadoop2.7.tgz -C /usr/cloud302
mv /usr/cloud302/spark-2.4.4-bin-hadoop2.7 /usr/cloud302/spark
```

## 添加环境变量

```
vim /etc/profile
```

```
export SPARK_HOME=/usr/cloud302/spark
export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH

```

```
source /etc/profile

```



# 配置 MongoDB(Replica Set)

## 解压安装包

```
tar -zxvf mongodb-linux-x86_64-rhel70-4.2.1.tgz -C /usr/cloud302
mv /usr/cloud302/mongodb-linux-x86_64-rhel70-4.2.1 /usr/cloud302/mongodb
```

## 添加环境变量

```
vim /etc/profile
```

```
export MONGODB_HOME=/usr/cloud302/mongodb
export PATH=${MONGODB_HOME}/bin:$PATH

```

```
source /etc/profile

```

## 在 `cloud3`、`cloud4`、`cloud5` 上，配置 `MongoDB(Replica Set)`

1. 在 `cloud3`、`cloud4`、`cloud5` 上，创建数据文件夹
```
mkdir -p $MONGODB_HOME/data/db
```

2. 在 `cloud3`、`cloud4`、`cloud5` 上，配置 `mongo.conf` 文件

```
systemLog:
    destination: file
    path: "/usr/cloud302/mongodb/mongo.log"
    logAppend: true
storage:
    dbPath: "/usr/cloud302/mongodb/data/db"
    journal:
        enabled: true
replication:
    replSetName: "cloud"
processManagement:
    fork: true
net:
    bindIp: 0.0.0.0
    port: 8018
setParameter:
    enableLocalhostAuthBypass: false

```

3. 在 `cloud3` 上，设置集群和创建帐号

```
mongo -host cloud3:8018

```

```
use admin
rs.initiate({_id: 'cloud', members:[{_id: 0, host: 'cloud3:8018'}, {_id: 2, host: 'cloud4:8018'}, {_id: 2, host: 'cloud4:8018'}]})

```

4. 在 `MongoDB` 集群上，创建帐号

```
mongo -host cloud/cloud3:8018,cloud4:8018,cloud5:8018
```
```
db.createUser({user: "admin", pwd: "123456", roles: [{ role: "userAdminAnyDatabase", db: "admin"}]})
```



# 配置 Redis


## 安装 `Redis`

```

yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel
tar -zxvf redis-5.0.5.tar.gz -C /usr/cloud302/
mv /usr/cloud302/redis-5.0.5/ /usr/cloud302/redis
make
```

## 添加环境变量

```
vim /etc/profile
```

```
export REDIS_HOME=/usr/cloud302/redis
export PATH=${REDIS_HOME}/src:$PATH

```

## 配置 `redis.conf` 文件

```
vim $REDIS_HOME/redis.conf
```

```
daemonize yes
dir /usr/cloud302/redis/
```



# 配置 uWSGI

## 进入 `Django` 虚拟环境

```
cd /usr/cloud302/moviesite
virtualenv . -p python3
source bin/activate
```

## 安装 `uWSGI` 所需的依赖

```
yum install zlib-devel bzip2-devel pcre-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel python3-devel

```

## 安装 `python` 库

```
pip install uwsgi

```

## 配置 `uwsgi.ini` 文件

```
vim uwsgi.ini
```

```
[uwsgi]
http=0:8005
chdir=/usr/cloud302/moviesite
wsgi-file=moviesite/wsgi.py
daemonize=/usr/cloud302/moviesite/uwsgi.log
pidfile=/usr/cloud302/moviesite/uwsgi.pid

```



# 配置 Nginx


## 安装 `Nginx`

```

yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel
tar -zxvf nginx-1.16.1.tar.gz
cd nginx-1.16.1
./configure --prefix=/usr/cloud302/nginx
make
make install
```

## 添加环境变量

```
vim /etc/profile
```

```
export NGINX_HOME=/usr/cloud302/nginx
export PATH=${NGINX_HOME}/sbin:$PATH

```

```
source /etc/profile
```

## 配置 `ngnix.conf` 文件

```
vim $NGINX_HOME/conf/nginx.conf
```

```
user root;
worker_processes  1;

events {
    worker_connections  1024;
}


http {
    upstream cloud{
        server cloud4:8005 weight=3;
        server cloud5:8005 weight=3;
    }

    sendfile        on;
    keepalive_timeout  65;

    server {
        listen       8006;
        server_name  localhost;
        location / {
            proxy_pass http://cloud;
        }
        location /static{
        alias /usr/cloud302/moviesite/static_new;
        }
    }
}
```
